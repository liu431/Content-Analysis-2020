{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Li Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig \n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud_2020\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample.csv\")\n",
    "# Predict positive reviews\n",
    "df['label'] = np.where(df['overall']>3, 1, 0)\n",
    "# Only use the first 100 charatcers\n",
    "df['sentence'] = [i[:100] for i in df['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2026\n",
       "0     541\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7892481495909622"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2026/(541+2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to balance the classes\n",
    "# Counts of two classes\n",
    "count_class_1, count_class_0 = df['label'].value_counts()\n",
    "\n",
    "# Divide the dataframe by class\n",
    "df_class_0 = df[df['label'] == 0]\n",
    "df_class_1 = df[df['label'] == 1]\n",
    "\n",
    "# Sample count_class_0 amount of observations from df_class_1\n",
    "df_class_1_under = df_class_1.sample(count_class_0, random_state=10)\n",
    "\n",
    "# Combine the two dataframes\n",
    "df_sample = pd.concat([df_class_1_under, df_class_0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    541\n",
       "0    541\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had one of these keyboards (this is my second) and only bought another as I accidentally spilled c'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.iloc[1]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('sample_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0526 20:46:26.965925 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'as', 'a', 'software', 'developer', ',', 'i', 'am', 'literally', 'attached', 'to', 'my', 'keyboard', '.', 'on', 'a', 'typical', 'day', ',', 'i', \"'\", 'll', 'spend', 'any', '##w', '##h', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2310/2310 [==============================] - 7s 3ms/step - loss: 0.5227 - accuracy: 0.7944\n",
      "Epoch 2/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5160 - accuracy: 0.7944: 2s - loss:\n",
      "Epoch 3/10\n",
      "2310/2310 [==============================] - 7s 3ms/step - loss: 0.5110 - accuracy: 0.7944\n",
      "Epoch 4/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5106 - accuracy: 0.7944\n",
      "Epoch 5/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5096 - accuracy: 0.7944\n",
      "Epoch 6/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5096 - accuracy: 0.7944\n",
      "Epoch 7/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5064 - accuracy: 0.7944\n",
      "Epoch 8/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5109 - accuracy: 0.7944\n",
      "Epoch 9/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5088 - accuracy: 0.7944\n",
      "Epoch 10/10\n",
      "2310/2310 [==============================] - 6s 3ms/step - loss: 0.5104 - accuracy: 0.7944\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict with re-sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df_sample.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df_sample.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0526 20:48:21.682268 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'i', 'have', 'used', 'an', 'er', '##gon', '##omic', 'keyboard', 'since', 'about', '1996', '&', 'just', 'love', 'it', '.', 'i', 'can', \"'\", 't', 'type', 'on', 'a', 'standard', 'key', '##bo', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6976 - accuracy: 0.4810\n",
      "Epoch 2/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6935 - accuracy: 0.4882\n",
      "Epoch 3/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6941 - accuracy: 0.4954\n",
      "Epoch 4/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.4954\n",
      "Epoch 5/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 6/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6934 - accuracy: 0.4995\n",
      "Epoch 7/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6934 - accuracy: 0.4707\n",
      "Epoch 8/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6934 - accuracy: 0.4882\n",
      "Epoch 9/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 10/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6935 - accuracy: 0.4779\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6942 - accuracy: 0.4923\n",
      "Epoch 2/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6942 - accuracy: 0.4748\n",
      "Epoch 3/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6941 - accuracy: 0.4902\n",
      "Epoch 4/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6938 - accuracy: 0.4974\n",
      "Epoch 5/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6931 - accuracy: 0.5026\n",
      "Epoch 6/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.4985\n",
      "Epoch 7/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6932 - accuracy: 0.4954\n",
      "Epoch 8/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6934 - accuracy: 0.5015\n",
      "Epoch 9/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.5015\n",
      "Epoch 10/10\n",
      "973/973 [==============================] - 3s 3ms/step - loss: 0.6932 - accuracy: 0.5015: 1s -\n"
     ]
    }
   ],
   "source": [
    "units = 100\n",
    "model_lstm2 = Sequential()\n",
    "model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm2.add(LSTM(unit))\n",
    "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_lstm2 = model_lstm2.fit(train_inputs, train_labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 20:24:13.473481 42180 configuration_utils.py:283] loading configuration file model_save\\config.json\n",
      "I0524 20:24:13.476473 42180 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0524 20:24:13.478493 42180 modeling_utils.py:648] loading weights file model_save\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# model trained in Colab\n",
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df2 = pd.read_csv('sample_new.csv')\n",
    "df2 = df2[df2['reviewText'].notnull()]\n",
    "\n",
    "# Predict positive reviews\n",
    "df2['label'] = np.where(df2['overall']>3, 1, 0)\n",
    "# Only use the first 200 charatcers\n",
    "df2['sentence'] = [i[:200] for i in df2['reviewText']]\n",
    "\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df2.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df2.sentence.values\n",
    "labels = df2.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_1, count_class_0 = df2['label'].value_counts()\n",
    "\n",
    "# Divide the dataframe by class\n",
    "df2_class_0 = df2[df2['label'] == 0]\n",
    "df2_class_1 = df2[df2['label'] == 1]\n",
    "\n",
    "# Sample count_class_0 amount of observations from df_class_1\n",
    "df2_class_1_under = df2_class_1.sample(count_class_0, random_state=10)\n",
    "\n",
    "# Combine the two dataframes\n",
    "df2_sample = pd.concat([df2_class_1_under, df2_class_0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 4,624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 20:34:28.117846 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 20:54:52.348126 42180 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0524 20:54:52.350086 42180 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0524 20:54:52.570185 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0524 20:54:52.880735 42180 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-config.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\437d6b3001e14ea1853bcee09a1b2557f230862c5a03d3ebd78a4cdb94a79020.879dd190c02fa5a86beee425ffb5735d066a7b5f2f2127d5d16ce06bba536566\n",
      "I0524 20:54:52.886702 42180 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0524 20:54:53.145931 42180 filelock.py:274] Lock 1433816336536 acquired on C:\\Users\\lliu9/.cache\\torch\\transformers\\57ded08a298ef01c397973781194aa0abf6176e6f720f660a2b93e8199dc0bc7.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n",
      "I0524 20:54:53.149924 42180 file_utils.py:436] https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json not found in cache or force_download set to True, downloading to C:\\Users\\lliu9\\.cache\\torch\\transformers\\tmpjhcom3te\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043aae2569d4549b15d1d49ac939f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 20:54:53.475383 42180 file_utils.py:440] storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json in cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\57ded08a298ef01c397973781194aa0abf6176e6f720f660a2b93e8199dc0bc7.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0524 20:54:53.482363 42180 file_utils.py:443] creating metadata file for C:\\Users\\lliu9/.cache\\torch\\transformers\\57ded08a298ef01c397973781194aa0abf6176e6f720f660a2b93e8199dc0bc7.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0524 20:54:53.485355 42180 filelock.py:318] Lock 1433816336536 released on C:\\Users\\lliu9/.cache\\torch\\transformers\\57ded08a298ef01c397973781194aa0abf6176e6f720f660a2b93e8199dc0bc7.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331.lock\n",
      "I0524 20:54:53.487350 42180 modelcard.py:161] loading model card file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\57ded08a298ef01c397973781194aa0abf6176e6f720f660a2b93e8199dc0bc7.455d944f3d1572ab55ed579849f751cf37f303e3388980a42d94f7cd57a4e331\n",
      "I0524 20:54:53.488348 42180 modelcard.py:179] Model card: {\n",
      "  \"caveats_and_recommendations\": {},\n",
      "  \"ethical_considerations\": {},\n",
      "  \"evaluation_data\": {},\n",
      "  \"factors\": {},\n",
      "  \"intended_use\": {},\n",
      "  \"metrics\": {},\n",
      "  \"model_details\": {},\n",
      "  \"quantitative_analyses\": {},\n",
      "  \"training_data\": {}\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 20:54:53.774923 42180 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/distilbert-base-uncased-finetuned-sst-2-english-pytorch_model.bin from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\dd75d79590ca5f697e98574177cada739798baad3b771fd8f500e08f9f8b78cf.461f3160566473d3587f9f4776a5131b1ed527b0d5fccb4b5f06003f457154bc\n"
     ]
    }
   ],
   "source": [
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997735023498535"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['sentence_sentiment'] = [nlp_sentiment(i)[0]['label'] for i in df_sample['sentence']]\n",
    "df_sample['sentence_score'] = [nlp_sentiment(i)[0]['score'] for i in df_sample['sentence']]\n",
    "df_sample['sentence_sentiment'] = np.where(df_sample['sentence_sentiment']=='POSITIVE', 1, -1)\n",
    "df_sample['sentence_score'] = df_sample['sentence_score']*df_sample['sentence_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(df_sample['sentence_score'])\n",
    "plt.title(\"Sentiments of First 200 Characters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:51.344599 42180 filelock.py:274] Lock 1434506739272 acquired on C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
      "I0524 21:32:51.345613 42180 file_utils.py:436] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to C:\\Users\\lliu9\\.cache\\torch\\transformers\\tmpa5tpj809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51c9c55374f4983bd77ae2ab74a1395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=665, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:51.608154 42180 file_utils.py:440] storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "I0524 21:32:51.612142 42180 file_utils.py:443] creating metadata file for C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "I0524 21:32:51.615134 42180 filelock.py:318] Lock 1434506739272 released on C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
      "I0524 21:32:51.617128 42180 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "I0524 21:32:51.619137 42180 configuration_utils.py:321] Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:51.896983 42180 filelock.py:274] Lock 1434506388928 acquired on C:\\Users\\lliu9/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
      "I0524 21:32:51.899973 42180 file_utils.py:436] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to C:\\Users\\lliu9\\.cache\\torch\\transformers\\tmpx5ifuy9_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4194e248079f48e4aa2738023e61d181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:52.382320 42180 file_utils.py:440] storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0524 21:32:52.385312 42180 file_utils.py:443] creating metadata file for C:\\Users\\lliu9/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0524 21:32:52.388303 42180 filelock.py:318] Lock 1434506388928 released on C:\\Users\\lliu9/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:52.611953 42180 filelock.py:274] Lock 1434506456424 acquired on C:\\Users\\lliu9/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "I0524 21:32:52.614935 42180 file_utils.py:436] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to C:\\Users\\lliu9\\.cache\\torch\\transformers\\tmp_1c63ut2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcb715a216b4dd0a6d03c94e011d101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:53.119925 42180 file_utils.py:440] storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0524 21:32:53.122868 42180 file_utils.py:443] creating metadata file for C:\\Users\\lliu9/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0524 21:32:53.125861 42180 filelock.py:318] Lock 1434506456424 released on C:\\Users\\lliu9/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "I0524 21:32:53.126857 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0524 21:32:53.127855 42180 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:32:53.449318 42180 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "I0524 21:32:53.450316 42180 configuration_utils.py:321] Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0524 21:32:53.868472 42180 filelock.py:274] Lock 1434506701792 acquired on C:\\Users\\lliu9/.cache\\torch\\transformers\\d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
      "I0524 21:32:53.873467 42180 file_utils.py:436] https://cdn.huggingface.co/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\lliu9\\.cache\\torch\\transformers\\tmpjox2en45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0ce4c994dd45209b11084442bf1388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=548118077, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:33:36.310857 42180 file_utils.py:440] storing https://cdn.huggingface.co/gpt2-pytorch_model.bin in cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "I0524 21:33:36.315841 42180 file_utils.py:443] creating metadata file for C:\\Users\\lliu9/.cache\\torch\\transformers\\d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "I0524 21:33:36.319829 42180 filelock.py:318] Lock 1434506701792 released on C:\\Users\\lliu9/.cache\\torch\\transformers\\d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
      "I0524 21:33:36.320829 42180 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at C:\\Users\\lliu9/.cache\\torch\\transformers\\d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 21:33:40.358036 42180 modeling_utils.py:741] Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"output_gpt_review\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"output_gpt_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df_sample['sentence'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text2, test_text2 = train_test_split(df2_sample['sentence'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text', header=None, index=None, sep=' ', mode='a')\n",
    "test_text.to_frame().to_csv(r'test_text', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text2.to_frame().to_csv(r'train_text2', header=None, index=None, sep=' ', mode='a')\n",
    "test_text2.to_frame().to_csv(r'test_text2', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trained in Colab\n",
    "tokenizer_review = AutoTokenizer.from_pretrained(\"output_gpt_review\")\n",
    "model_review = AutoModelWithLMHead.from_pretrained(\"output_gpt_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I like it, but\"\n",
    "\n",
    "input = tokenizer_review.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_review.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_review.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== GENERATED SEQUENCE 1 ===\n",
    "\n",
    "i like it but this router doesn't do it for me. I've been using it for years now, but it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cosine\n",
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 22:56:04.572855 42180 configuration_utils.py:283] loading configuration file output_gpt_review2\\config.json\n",
      "I0524 22:56:04.574849 42180 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0524 22:56:04.576843 42180 modeling_utils.py:648] loading weights file output_gpt_review2\\pytorch_model.bin\n",
      "I0524 22:56:08.378713 42180 modeling_utils.py:741] Weights of RobertaModel not initialized from pretrained model: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0524 22:56:08.379711 42180 modeling_utils.py:747] Weights from pretrained model not used in RobertaModel: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
      "I0524 22:56:08.380677 42180 tokenization_utils.py:929] Model name 'output_gpt_review2' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'output_gpt_review2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0524 22:56:08.383670 42180 tokenization_utils.py:958] Didn't find file output_gpt_review2\\added_tokens.json. We won't load it.\n",
      "I0524 22:56:08.385711 42180 tokenization_utils.py:1013] loading file output_gpt_review2\\vocab.json\n",
      "I0524 22:56:08.386688 42180 tokenization_utils.py:1013] loading file output_gpt_review2\\merges.txt\n",
      "I0524 22:56:08.387658 42180 tokenization_utils.py:1013] loading file None\n",
      "I0524 22:56:08.387658 42180 tokenization_utils.py:1013] loading file output_gpt_review2\\special_tokens_map.json\n",
      "I0524 22:56:08.388656 42180 tokenization_utils.py:1013] loading file output_gpt_review2\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "roberta_m2_model_embedding = RobertaModel.from_pretrained('output_gpt_review2')\n",
    "roberta_m2_tokenizer = RobertaTokenizer.from_pretrained('output_gpt_review2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a very nice keyboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_m1_model_embedding, roberta_m1_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_m2_model_embedding, roberta_m2_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't see much difference in the two heatmap visualizations. This suggests the two sets of product reviews are quite similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
